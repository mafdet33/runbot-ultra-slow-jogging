# scrape_mohw_selenium.py
import time
import json
import re
import os
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.common.by import By

# -----------------------
# æ¸…ç†æ–‡å­—
# -----------------------
def clean(text):
    text = re.sub(r"\s+", " ", text)
    return text.strip()


# -----------------------
# Selenium æŠ“å–®ç¯‡å…¨æ–‡
# -----------------------

def fetch_article(url):
    print(f"ğŸ¥— æŠ“å–æ–‡ç« ï¼š{url}")

    options = webdriver.ChromeOptions()
    options.add_argument("--headless")
    options.add_argument("--disable-gpu")

    driver = webdriver.Chrome(
        service=Service(ChromeDriverManager().install()),
        options=options
    )

    driver.get(url)
    time.sleep(2)

    # å˜—è©¦æŠ“æ–‡ç« å…§å®¹
    candidates = [
        "#page-top",
        ".innerPage",
        ".content",
        "#ContentPlaceHolder1_pnlContent",
        "article",
    ]

    text = ""
    for c in candidates:
        try:
            el = driver.find_element(By.CSS_SELECTOR, c)
            raw = el.text
            if len(raw) > len(text):
                text = raw
        except:
            continue

    if not text or len(text) < 50:
        text = driver.find_element(By.TAG_NAME, "body").text

    # å˜—è©¦æŠ“æ¨™é¡Œ
    title_candidates = [
        "h1",
        "h2",
        ".page-main-title",
        ".title",
    ]

    title = ""
    for c in title_candidates:
        try:
            t = driver.find_element(By.CSS_SELECTOR, c).text
            if t and len(t) > len(title):
                title = t
        except:
            continue

    # fallback
    if not title:
        title = driver.title

    driver.quit()

    return clean(title), clean(text)

# -----------------------
# åŸºæœ¬ NLP åˆ†ç¾¤
# -----------------------
def classify(text):
    text = text.lower()
    if any(k in text for k in ["è¿·æ€", "éŒ¯èª¤", "çœŸå‡", "é—¢è¬ ", "æµè¨€"]):
        return "é—¢è¬ "
    if any(k in text for k in ["è€äºº", "é•·è€…", "éŠ€é«®", "è·Œå€’"]):
        return "è€äººå¥åº·"
    if any(k in text for k in ["å…’ç«¥", "å°å­©", "å¹¼å…’"]):
        return "å…’ç«¥å¥åº·"
    if any(k in text for k in ["é‹å‹•", "æ­¥é“", "é«”èƒ½", "å¥èµ°", "æ´»å‹•é‡"]):
        return "é‹å‹•ä¿ƒé€²"
    if any(k in text for k in ["é£²é£Ÿ", "ç‡Ÿé¤Š", "è„‚è‚ª", "ç†±é‡"]):
        return "é£²é£Ÿèˆ‡ç‡Ÿé¤Š"
    return "å…¶ä»–"


# -----------------------
# æŠ“æŸåˆ†é¡æœ€æ–° 10 ç¯‡æ–‡ç«  pid
# -----------------------
def fetch_latest_pids(list_url, limit=10):
    print(f"\nğŸ“„ æŠ“å–åˆ—è¡¨é ï¼š{list_url}")

    options = webdriver.ChromeOptions()
    options.add_argument("--headless=new")
    driver = webdriver.Chrome(
        service=Service(ChromeDriverManager().install()),
        options=options
    )

    driver.get(list_url)
    time.sleep(1)

    # ğŸ” æŠ“æ‰€æœ‰ Detail é€£çµ
    elems = driver.find_elements(By.CSS_SELECTOR, "a")
    urls = []
    for e in elems:
        try:
            href = e.get_attribute("href")
            if href and "Detail.aspx" in href:
                urls.append(href)
        except:
            continue

    driver.quit()

    # å»é‡ + åªå–æœ€æ–° limit ç¯‡
    urls = list(dict.fromkeys(urls))  
    urls = urls[:limit]
    print(f"âœ“ æŠ“åˆ° {len(urls)} ç¯‡")
    return urls


# -----------------------
# ä¸»æµç¨‹ï¼šæ¯åˆ†é¡æŠ“ 10 ç¯‡
# -----------------------
def run():

    CATEGORY_LIST = {
        "è€äººå¥åº·": "https://www.hpa.gov.tw/Pages/List.aspx?nodeid=4625",
        "é—¢è¬ ": "https://www.hpa.gov.tw/Pages/List.aspx?nodeid=127",
        "å…’ç«¥å¥åº·": "https://www.hpa.gov.tw/Pages/List.aspx?nodeid=4477",
        "é‹å‹•ä¿ƒé€²": "https://www.hpa.gov.tw/Pages/List.aspx?nodeid=40",
        "é£²é£Ÿèˆ‡ç‡Ÿé¤Š": "https://www.hpa.gov.tw/Pages/List.aspx?nodeid=37"
    }

    results = {}

    for cat, list_url in CATEGORY_LIST.items():
        print(f"\n=========================")
        print(f"ğŸ“š åˆ†é¡ï¼š{cat}")
        print(f"=========================")

        urls = fetch_latest_pids(list_url, limit=10)
        results[cat] = []

        for url in urls:
            try:
                title, content = fetch_article(url)
                results[cat].append({
                    "title": title,
                    "content": content,
                    "category": cat,
                    "url": url
                })
            except Exception as e:
                print(f"âš ï¸ æŠ“å–å¤±æ•—ï¼š{url}, error={e}")

    os.makedirs("content", exist_ok=True)
    with open("content/mohw_grouped.json", "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    print("\nğŸ‰ å·²ç”¢ç”Ÿ content/mohw_grouped.json")


if __name__ == "__main__":
    run()